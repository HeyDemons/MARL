{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练后的策略：\n",
      "↓ ← ← ← X\n",
      "↓ ← ← X ↓\n",
      "↓ ← X ↓ ↓\n",
      "↓ X → ↓ ↓\n",
      "→ → → → G\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 定义网格世界环境\n",
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        self.n_rows = 5\n",
    "        self.n_cols = 5\n",
    "        self.start = (0, 0)\n",
    "        self.goal = (4, 4)\n",
    "        # 定义障碍物\n",
    "        self.obstacles = [(2, 2), (3, 1),(1, 3),(0, 4)]\n",
    "        self.state = self.start\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"重置环境至起点\"\"\"\n",
    "        self.state = self.start\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        执行动作\n",
    "        动作：0: 上, 1: 下, 2: 左, 3: 右, 4: 原地不动\n",
    "        返回：下一个状态, 奖励, 是否结束\n",
    "        \n",
    "        为防止出界，所有移动动作都会在边界处被限制在有效范围内。\n",
    "        \"\"\"\n",
    "        x, y = self.state\n",
    "        \n",
    "        # 根据动作计算下一个状态，并利用边界检查确保不出界\n",
    "        if action == 0:  # 上\n",
    "            next_state = (max(x - 1, 0), y)\n",
    "        elif action == 1:  # 下\n",
    "            next_state = (min(x + 1, self.n_rows - 1), y)\n",
    "        elif action == 2:  # 左\n",
    "            next_state = (x, max(y - 1, 0))\n",
    "        elif action == 3:  # 右\n",
    "            next_state = (x, min(y + 1, self.n_cols - 1))\n",
    "        elif action == 4:  # 原地不动\n",
    "            next_state = (x, y)\n",
    "        else:\n",
    "            next_state = self.state\n",
    "\n",
    "        # 如果试图出界（虽然由于边界检查不可能发生），则保持原状态\n",
    "        if next_state == self.state and action != 4:\n",
    "            # 可以选择额外惩罚出界动作，但此处统一奖励为 -1\n",
    "            reward = -2\n",
    "            done = False\n",
    "        # 判断是否进入障碍物\n",
    "        elif next_state in self.obstacles:\n",
    "            reward = -10\n",
    "            # 撞到障碍物后保持原状态\n",
    "            next_state = self.state\n",
    "            done = False\n",
    "        elif next_state == self.goal:\n",
    "            reward = 10\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -1\n",
    "            done = False\n",
    "\n",
    "        self.state = next_state\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def get_possible_actions(self):\n",
    "        \"\"\"返回所有可能的动作（包含原地不动）\"\"\"\n",
    "        return [0, 1, 2, 3, 4]\n",
    "\n",
    "\n",
    "# Q-learning 算法实现\n",
    "def train_agent(env, episodes=1000, alpha=0.1, gamma=0.9, epsilon=0.2):\n",
    "    # 初始化 Q 表：每个状态下的 5 个动作的 Q 值初始化为 0\n",
    "    Q = {}\n",
    "    for i in range(env.n_rows):\n",
    "        for j in range(env.n_cols):\n",
    "            Q[(i, j)] = np.zeros(5)\n",
    "    \n",
    "    rewards_per_episode = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # ε-greedy 策略选择动作\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = random.choice(env.get_possible_actions())\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "            \n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Q-learning 更新公式\n",
    "            best_next_action = np.argmax(Q[next_state])\n",
    "            Q[state][action] += alpha * (reward + gamma * Q[next_state][best_next_action] - Q[state][action])\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        rewards_per_episode.append(total_reward)\n",
    "        # 可选：打印每一集的总奖励，观察收敛过程\n",
    "        # print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "    \n",
    "    return Q, rewards_per_episode\n",
    "\n",
    "\n",
    "# 输出策略函数：用箭头表示最优动作\n",
    "def print_policy(Q, env):\n",
    "    actions_map = {0: '↑', 1: '↓', 2: '←', 3: '→', 4: '•'}\n",
    "    grid_policy = []\n",
    "    for i in range(env.n_rows):\n",
    "        row = []\n",
    "        for j in range(env.n_cols):\n",
    "            if (i, j) in env.obstacles:\n",
    "                row.append('X')  # 障碍物用 X 表示\n",
    "            elif (i, j) == env.goal:\n",
    "                row.append('G')  # 目标状态用 G 表示\n",
    "            else:\n",
    "                action = np.argmax(Q[(i, j)])\n",
    "                row.append(actions_map[action])\n",
    "        grid_policy.append(row)\n",
    "    \n",
    "    for row in grid_policy:\n",
    "        print(' '.join(row))\n",
    "\n",
    "\n",
    "# 主程序入口\n",
    "if __name__ == '__main__':\n",
    "    env = GridWorld()\n",
    "    Q, rewards = train_agent(env, episodes=10000, alpha=0.1, gamma=0.9, epsilon=0.2)\n",
    "    print(\"训练后的策略：\")\n",
    "    print_policy(Q, env)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
